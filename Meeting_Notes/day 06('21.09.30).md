
# '21. 09. 30 NLP 10조 피어세션 정리'

### 오늘의 BEST RESULT
총 제출횟수 : 7 <br>
Baseline Model -> 18th / 19<br>
micro F1 : 63.389<br>
auprc : 69.895<br>

## 내용
## EDA를 어떻게 하면 좋을까
- Optuna 등 하이퍼파라미터 튜닝
- kfold 모델선택
- 모델 선택 후 앙상블 구현
- Data Augmentaion으로 번역기 이용
## 새로운 방법
- 기존은 SUB, OBJ를 이용해 Relation을 뽑지만, SUB랑 Relation을 이용해서 OBJ를 Extraction 해보자.<br>
Orginal : x1 + x2 = y<br>
Changed :x1 +  y = x2

(SUB, OBJ의 Type을 넣어준다.)

Entity tag를 양 옆에 붙이니까 성능이 잘 나온다

한국어 객체명을 태그로 추가하기

## 전처리 해야 할 것들 목록정리 

1. 처음하고 맨 끝 따옴표 지워주기
2. 다양한 큰 따옴표 기호 모두 하나로 치환 (큰따옴표 두개 포함) " ' ' " " " > " "
3. 다양한 괄호 전부 하나로 치환
4. 기타 문자 제거
5. 점 개수 하나로 변경 ( 중앙점은 쉼표로 치환 )
6. 

Augmentation에 관한 논문,

GPT-2를 이용한 방법. (Huggingface이용, BERT도 가능하지 않을까 ? ex: mask 채우기 )



https://huggingface.co/transformers/main_classes/trainer.html?highlight=hyperparameter_search


Launch an hyperparameter search using optuna or Ray Tune or SigOpt. The optimized quantity is determined by compute_objective, which defaults to a function returning the evaluation loss when no metric is provided, the sum of all metrics otherwise.





