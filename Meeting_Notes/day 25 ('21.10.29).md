# '21. 10. 29 NLP 10조 피어세션 정리'

## Dense Retriever

**Base Line Code가 학습을 전혀 하지 못하고 있다...** (킹갓지호 says)
Dense Retriever에 있는 다양한 변수들을 바꿔서 한 번 실험해보았다.

- Negative Sampling할 때, num_neg를 3에서 10으로 변경
    - OOM 발생
- num_neg를 5정도로 수정 -> 그럼에도 결과가 처참함.
- Train data를 2개로 줄여봤다 -> 그럼에도 결과가 처참함
    - 학습이 아예 안되고 있다는 소리. (코드 수정 필요)

1순위 **Elastic Search**와 2순위 **Dense Retriever**를 병행해서 사용하는 방법을 찾아봐야겠다.

## T5 Model (Closed book)

T5 관련 논문 읽고 있는 중 (성능을 중심으로)
- EM Score를 맞춰보면 별로 높지 않을 것 같음.
    - 중복 정답을 허용한다고 하더라도, 그 기준이 널널하진 않을 듯...?
    - Closed Book은 Answer을 Search하는 것이 아니라 Generate하기 때문에 EM이 필연적으로 낮을 수 밖에 없다고 생각

- 따라서 전체 모델을 T5로 바꾸기 보다는, Reader만 대체하는 방법으로 사용할 수 있지 않을까 생각.

## 질의응답

1. Top-K로 K개의 문서를 뽑은 후에, K개의 문서에서 정답을 어떻게 찾는가?

- 두 가지 방법이 있는데, **(1)** K개의 문서를 concat 한 다음에 그 중에 정답을 찾는다. **(2)** K개의 문서에서 각각의 정답을 찾은 후에, 확률값을 비교하여 최선의 값을 찾는다.
- Base Line Code에서는 첫 번째 방법을 이용해서 찾는다.

2. 아직 학습되지 않은 상태의 passage Encoder에 같은 값들을 넣어봤는데 결과가 다르게 나왔다. 왜 그러지..?
- 입력값 (2번째 값부터 6번째 값까지는 전부 같은 텐서임) 
![](https://i.imgur.com/bz60bpZ.png)

- 출력값 (2번째 값부터 6번째 값까지는 다 다른 결과가 나옴) 
![](https://i.imgur.com/BCMERQV.png)


## 아이디어

1. Ensenble을 할 때, Retriever의 결과 역시 고려하여 하는 방법을 고려해볼 수 있겠다.

- Retriever가 질문과 가장 관련성 높은 5개 문서 출력
• Reader는 5개 문서를 읽고 답변 예측
• Reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용함





