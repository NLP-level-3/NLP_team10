# '21. 12. 01 NLP 10조 피어세션 정리'

## 대회 진행 상황

- 지호: LR scheduler(CyclicLR) optuna, augmentation optuna 하는 중
- 범찬: 모델 150epochs로 train 시키고 있고, KD로 pruning 진행중
- 민성: 모델 학습 후 pruning 후 다시 학습하는 중, 이미지 사이즈 96으로 조정
- 수홍: augmentation을 albumentation으로 바꿔서 점수 향상 확인함, inference 코드에 albumentation 적용은 했으나 대회 규정 상 가능한지 알아보는 중, 범찬님 모델을 그대로 돌렸는데 f1스코어 0.65까지 200epoch 필요하는데 이유를 모르겠음

## f1 score가 0.65에서 더 올라가지 않는 점

- efficientnet이나 custom 모델이나 여러 모델들이 0.65에서 올라가지 않는다는 점을 해결해야 함
- augmentation 등 데이터를 건드리는게 좋을듯
- CustomCriterion의 문제 발견, data_path를 os.path.join(data_path, "train")으로 변경